{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "import const\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert categorical data to numbers\n",
    "(Saves approx 25% memory usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read all data\n",
    "a=time.time()\n",
    "b=pd.read_csv(os.path.join(c.BASE_PATH,'train_categorical.csv'), dtype='object')\n",
    "print time.time()-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Batch wise stripping of elements\n",
    "i_s = [0, 500, 1000, 1500, 2000]\n",
    "i_e = [500, 1000, 1500, 2000, 2141]\n",
    "\n",
    "for s,e in zip(i_s, i_e):\n",
    "    print s, e\n",
    "    #a=time.time()\n",
    "    b.iloc[:,s:e]=b.iloc[:,s:e].apply(lambda x: x.str.lstrip('T'), axis=0)\n",
    "    #print (time.time()-a)\n",
    "    #print (time.time()-a)/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.to_csv(os.path.join(c.BASE_PATH,'test_categorical_to_num.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convert to sparse and check memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def memory_usage(var):\n",
    "    ''' Returns estimate of memory usage using pickle trick '''\n",
    "    tmp_file_path = os.path.join(c.BASE_PATH, 'tmp_dump_file.pkl')\n",
    "    \n",
    "    with open(tmp_file_path,'wb') as f:\n",
    "        pickle.dump(var, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    file_size = os.path.getsize(tmp_file_path)/1024/1024\n",
    "    os.remove(tmp_file_path)\n",
    "    \n",
    "    return file_size\n",
    "\n",
    "\n",
    "def read_csv_as_sparse(csv_file, chuncksize=10000, verbose=False):\n",
    "    ''' Read csv and return sparse matrix'''\n",
    "    \n",
    "    print('Reading {} with size {} MB'.format(csv_file, os.path.getsize(csv_file)/1024/1024))\n",
    "    \n",
    "    # Check for columns to read\n",
    "    quick_scan = pd.read_csv(csv_file,\n",
    "                         nrows=1, \n",
    "                         dtype=np.float32)\n",
    "    \n",
    "    columns_names_used = list(quick_scan.columns)\n",
    "    \n",
    "    if 'Response' in quick_scan.columns:\n",
    "        if verbose:\n",
    "            print('Excluding Response column')\n",
    "        columns_names_used.remove('Response')\n",
    "    \n",
    "    \n",
    "    #print(len(columns_names_used))\n",
    "    #print(columns_names_used)\n",
    "    \n",
    "    # index_col is the first column of usecols, not the first column in the csv file\n",
    "    reader = pd.read_csv(csv_file,\n",
    "                         chunksize=chuncksize, \n",
    "                         dtype=np.float32,\n",
    "                         index_col=0,\n",
    "                         usecols=columns_names_used)\n",
    "    \n",
    "    \n",
    "    ids = pd.read_csv(csv_file, usecols=[0])\n",
    "    \n",
    "    if verbose:\n",
    "        for i,ch in enumerate(reader):\n",
    "            #print len(ch.columns)\n",
    "            #print ch.columns\n",
    "            \n",
    "            ch = ch.replace(0, 1e-5)\n",
    "            \n",
    "            if i==0:\n",
    "                csr = csr_matrix(ch.fillna(0))\n",
    "            else:\n",
    "                csr = vstack([csr, csr_matrix(ch.fillna(0))], format='csr')\n",
    "\n",
    "            if not i % 10:\n",
    "                print('Doing chunck: {} | status: {} elements'.format(i, csr.getnnz()))\n",
    "    else:\n",
    "        csr = vstack([csr_matrix(ch.fillna(0)) for ch in reader], format='csr')\n",
    "    \n",
    "    print('Sparse matrix has {} elements and {} MB memory usage'.format(csr.getnnz(),\n",
    "                                                                       memory_usage(csr)))\n",
    "    \n",
    "    return csr, ids, columns_names_used\n",
    "\n",
    "def read_csv_as_sparse_bin(csv_file, chuncksize=10000, verbose=False):\n",
    "    ''' Read csv and return sparse matrix'''\n",
    "    \n",
    "    print('Reading {} with size {} MB'.format(csv_file, os.path.getsize(csv_file)/1024/1024))\n",
    "    \n",
    "    # Check for columns to read\n",
    "    quick_scan = pd.read_csv(csv_file,\n",
    "                         nrows=1, \n",
    "                         dtype=np.float32)\n",
    "    \n",
    "    columns_names_used = list(quick_scan.columns)\n",
    "    \n",
    "    if 'Response' in quick_scan.columns:\n",
    "        if verbose:\n",
    "            print('Excluding Response column')\n",
    "        columns_names_used.remove('Response')\n",
    "    \n",
    "    \n",
    "    #print(len(columns_names_used))\n",
    "    #print(columns_names_used)\n",
    "    \n",
    "    # index_col is the first column of usecols, not the first column in the csv file\n",
    "    reader = pd.read_csv(csv_file,\n",
    "                         chunksize=chuncksize, \n",
    "                         dtype=np.float32,\n",
    "                         index_col=0,\n",
    "                         usecols=columns_names_used)\n",
    "    \n",
    "    \n",
    "    ids = pd.read_csv(csv_file, usecols=[0])\n",
    "    \n",
    "    if verbose:\n",
    "        for i,ch in enumerate(reader):\n",
    "            #print len(ch.columns)\n",
    "            #print ch.columns\n",
    "            \n",
    "            if i==0:\n",
    "                csr = csr_matrix(~ch.isnull())\n",
    "            else:\n",
    "                csr = vstack([csr, csr_matrix(~ch.isnull())], format='csr')\n",
    "\n",
    "            if not i % 1:\n",
    "                print('Doing chunck: {} | status: {} elements'.format(i, csr.getnnz()))\n",
    "    else:\n",
    "        csr = vstack([csr_matrix(~ch.isnull()) for ch in reader], format='csr')\n",
    "    \n",
    "    print('Sparse matrix has {} elements and {} MB memory usage'.format(csr.getnnz(),\n",
    "                                                                       memory_usage(csr)))\n",
    "    \n",
    "    return csr, ids, columns_names_used\n",
    "\n",
    "def read_last_column(csv_file):\n",
    "    ''' Reads last column in csv file'''\n",
    "    sample = pd.read_csv(os.path.join(c.BASE_PATH,csv_file), nrows=1)\n",
    "    \n",
    "    return pd.read_csv(os.path.join(c.BASE_PATH,csv_file), usecols=[0,sample.shape[1]-1], index_col=0)\n",
    "\n",
    "def convert_data_file_to_pickle(filename, verbose=False):\n",
    "    ''' Reads full csv file, converts to sparse and stores using pickle with metadata'''\n",
    "    \n",
    "    csr, ids, f_names = read_csv_as_sparse(os.path.join(c.BASE_PATH, filename + '.csv'), \n",
    "                                  chuncksize=100000,\n",
    "                                  verbose=verbose)\n",
    "    \n",
    "    print('Reading data as sparse took {}s'.format(time.time()-a))\n",
    "    \n",
    "    y = read_last_column(os.path.join(c.BASE_PATH, 'train_numeric.csv'))\n",
    "    \n",
    "    output = {'data': {'ids': ids, 'y': y, 'features': csr, 'feature_names': f_names},\n",
    "              'creation_date':datetime.datetime.now(),\n",
    "              'created_by': 'joostgp',\n",
    "              'script': 'convert_data_to_sparse'}\n",
    "    \n",
    "    save_path = os.path.join(c.BASE_PATH,filename + '.pkl')\n",
    "    \n",
    "    with open(save_path,'wb') as f:\n",
    "        pickle.dump(output, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print('Results stored in {}'.format(save_path))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_files = ['train_categorical_to_num',\n",
    "              'train_numeric',\n",
    "              'train_date',\n",
    "              'test_categorical_to_num',\n",
    "              'test_numeric',\n",
    "              'test_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert_data_file_to_pickle(data_files[1], verbose=True)\n",
    "#convert_data_file_to_pickle(data_files[4], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in data_files:\n",
    "     convert_data_file_to_pickle(f, verbose=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
